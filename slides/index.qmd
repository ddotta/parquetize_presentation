---
title: parquetize
subtitle: |
  **[Un package R\
  pour convertir des donn√©es\
  au format parquet]{.orange}**
author: Damien Dotta et Nicolas Chuche
slide-number: true
footer: |
  Pr√©sentation √† l'URDA
# uncomment for French presentations:
lang: fr-FR
# for blind readers:
slide-tone: false
# for @olevitt:
chalkboard: # press the B key to toggle chalkboard
  theme: whiteboard
# uncomment to use the multiplex mode:
#multiplex: true
format:
  # pick the light mode (onyxia-revealjs) or the dark mode (onyxia-dark-revealjs)
  onyxia-revealjs:
  #onyxia-dark-revealjs:
    output-file: index.html
controls: true
css: custom.css
from: markdown+emoji
---

## Fonctionnalit√©s de parquetize

::: {.callout-note}
Rappel sur une pr√©sentation du format parquet [ici](https://www.book.utilitr.org/03_fiches_thematiques/fiche_import_fichiers_parquet)
:::

- [**Convertir**]{.orange} des fichiers de diff√©rents formats [**en fichier parquet**]{.orange}

- Plus-value de parquetize : [**faciliter la vie des utilisateurs**]{.orange} et [**fournir un cadre**]{.orange} 
pour appliquer des r√®gles lors des conversions de fichiers.

- Les formats g√©r√©s : SAS, SPSS, Stata, csv, json, rds, fst, DBI...

## R√©sum√© des formats g√©r√©s

| Format  de fichier| Fonctions de parquetize                                                                   	|
|------------------	|-------------------------------------------------------------------------------------------	|
| SAS, SPSS, Stata 	| [table_to_parquet](https://ddotta.github.io/parquetize/reference/table_to_parquet.html)   	|
| csv              	| [csv_to_parquet](https://ddotta.github.io/parquetize/reference/csv_to_parquet.html)       	|
| rds              	| [rds_to_parquet](https://ddotta.github.io/parquetize/reference/rds_to_parquet.html)       	|
| fst              	| [fst_to_parquet](https://ddotta.github.io/parquetize/reference/fst_to_parquet.html)       	|
| sqlite, db, sdb 	| [sqlite_to_parquet](https://ddotta.github.io/parquetize/reference/sqlite_to_parquet.html) 	|
| DBI databases    	| [DBI_to_parquet](https://ddotta.github.io/parquetize/reference/DBI_to_parquet.html)       	|
| json             	| [json_to_parquet](https://ddotta.github.io/parquetize/reference/json_to_parquet.html)     	|

: Les formats support√©s par parquetize

## 1√®re d√©monstrastion avec des fichiers csv

```{.r code-line-numbers="3-8"}
# Conversion from a local csv file to a partitioned parquet file  :

csv_to_parquet(
  path_to_file = parquetize_example("region_2022.csv"),
  path_to_parquet = tempfile(fileext = ".parquet"),
  partition = "yes",
  partitioning =  c("REG")
)
#> Reading data...
#> Writing data...
#> ‚úî Data are available in parquet dataset under /tmp/RtmpMT4bYz/file157830a758fa.parquet
#> Writing data..
```

. . .

![](img/Insee_example_csv.gif)

## Documentation 

- Un [site web](https://ddotta.github.io/parquetize/) d√©di√© au package

- Une [documentation](https://ddotta.github.io/parquetize/reference/index.html) qui accompagne chaque fonction.

- Un [article](https://ddotta.github.io/parquetize/articles/aa-conversions.html) qui pr√©sente comment convertir un gros fichier lorsqu'on on est limit√© par la m√©moire vive de son espace de travail.

![](img/hex_parquetize.png){fig-align="center"}



## Plusieurs param√©trages possibles [1/2]

- **Trois options pour les conversions :**  
	:one: Convertir dans un [**fichier parquet unique**]{.orange}  
	:two: Convertir dans un r√©pertoire avec des [**fichiers parquets partitionn√©s**]{.orange}    
	:three: Convertir dans des [**fichiers parquets multiples**]{.orange}  
	
## Plusieurs param√©trages possibles [2/2]

- **Autre param√®tre** : [**s√©lectionner les variables**]{.orange} qui seront √† conserver dans le fichier parquetis√© (sans-doute pas utile pour l'URDA)

- **Autre param√®tre** : pour les fichiers volumineux, il est possible [**d'effectuer la conversion par "chunk"**]{.orange} soit par "bout" de table (cf. [la vignette](https://ddotta.github.io/parquetize/articles/aa-conversions.html)).

- **Autre param√®tre** : le [**mode de compression**]{.orange} choisi pour les fichiers parquet.

## `table_to_parquet` pour les fichiers SAS

Voici ses arguments :  

```{.r code-line-numbers="1-5,8,11-17"}
table_to_parquet(
  path_to_file,
  path_to_parquet,
  max_memory = NULL,
  max_rows = NULL,
  chunk_size = lifecycle::deprecated(),
  chunk_memory_size = lifecycle::deprecated(),
  columns = "all",
  by_chunk = lifecycle::deprecated(),
  skip = 0,
  partition = "no",
  encoding = NULL,
  chunk_memory_sample_lines = 10000,
  compression = "snappy",
  compression_level = NULL,
  ...
)
```

## SAS -> fichier parquet unique

Conversion d'un fichier SAS vers un fichier parquet [**unique**]{.orange}

```{.r code-line-numbers="1-4"}
table_to_parquet(
  path_to_file = system.file("examples","iris.sas7bdat", package = "haven"),
  path_to_parquet = tempfile(fileext = ".parquet")
)
#> Reading data...
#> Writing data...
#> ‚úî Data are available in parquet file under /tmp/RtmpMT4bYz/file15781027e4d0.parquet
```

## SAS -> fichier parquet multiple [1/2]

Conversion d'un fichier SAS vers un fichier parquet [**multiple**]{.orange} en fractionnant les donn√©es selon le nombre de lignes.

```{.r code-line-numbers="1-6"}
table_to_parquet(
  path_to_file = system.file("examples","iris.sas7bdat", package = "haven"),
  path_to_parquet = tempfile(),
  max_rows = 50, # On fractionne la table iris toutes les 50 lignes
  encoding = "utf-8"
)
#> Reading data...
#> Writing file1578a575aa3-1-50.parquet...
#> Reading data...
#> Writing file1578a575aa3-51-100.parquet...
#> Reading data...
#> Writing file1578a575aa3-101-150.parquet...
#> Reading data...
#> ‚úî Data are available in parquet dataset under /tmp/RtmpMT4bYz/file1578a575aa3/
```

Le fichier parquet peut ensuite √™tre reconstitu√© avec la fonction [rbind_parquet()](https://ddotta.github.io/parquetize/reference/rbind_parquet.html).

## SAS -> fichier parquet multiple [2/2]

Conversion d'un fichier SAS vers un fichier parquet [**multiple**]{.orange} en fractionnant les donn√©es en fonction de la consommation de m√©moire vive.

`table_to_parquet()` peut d√©terminer le nombre de lignes √† placer dans un fichier en fonction de la consommation de m√©moire avec l'argument `max_memory` exprim√© en Mb.

Le code suivant fractionne un fichier volumineux en plusieurs fichiers de 2 000 Mb (soit 250 Mo) :

```{.r}
  table_to_parquet(
  path_to_file = "myhugefile.sas7bdat",
  path_to_parquet = tempdir(),
  max_memory = 2000,
  encoding = "utf-8"
)
```

## Une r√©flexion √† avoir √† l'√©chelle du fichier

- A partir de quelle taille faut-il [**partitionner**]{.orange} les bases parquet ?
- Etre [**attentif aux temps machine**]{.orange} tr√®s importants sur AUS
- Etablir [**une √©chelle de priorit√©**]{.orange} dans les op√©rations de conversion

[^1]: https://arrow.apache.org/docs/r/articles/dataset.html#partitioning-performance-considerations

## parquetize et les bonnes pratiques

Le package parquetize peut √™tre l'occasion de mettre en place certaines bonnes pratiques :

- [**Pas de valeurs manquantes**]{.orange} dans les variables de partitionnement
- Privil√©gier le mode de compression [**snappy**]{.orange}.
- √âviter les fichiers partitionn√©s [**de moins de 20 MB ou de plus de 2 GB**]{.orange}. ^[https://arrow.apache.org/docs/r/articles/dataset.html#partitioning-performance-considerations]
- √âviter d‚Äôavoir [**plus de 10 000 partitions**]{.orange}.
 

## Un projet open source

<br>
Un package disponible sur le [CRAN](https://cran.r-project.org/web/packages/parquetize/index.html)

Le [d√©p√¥t github](https://github.com/ddotta/parquetize) du projet

Qui a besoin de vous pour √™tre enrichi ou pour nous faire remonter des probl√®mes => par [ici](https://github.com/ddotta/parquetize/issues) üöÄ

<br>

Les auteurs : [Damien Dotta](https://github.com/ddotta) et [Nicolas Chuche](https://github.com/nbc)
